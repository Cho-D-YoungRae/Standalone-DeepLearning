{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Lab5(MyStudy).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euB4iiZMleNy"
      },
      "source": [
        "[제가 미리 만들어놓은 이 링크](https://colab.research.google.com/github/heartcored98/Standalone-DeepLearning/blob/master/Lec4/Lab5_regularization_implemented.ipynb)를 통해 Colab에서 바로 작업하실 수 있습니다!  \n",
        "런타임 유형은 python3, GPU 가속 확인하기!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD4cIKKvKFCC"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import argparse\n",
        "import numpy as np\n",
        "import time"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29UainWPco7Y"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu753dPPKGkV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bc72f33-10eb-4065-e6a0-b5c6906ef1be"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainset, valset = torch.utils.data.random_split(trainset, [40000, 10000])\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "partition = {'train': trainset, 'val':valset, 'test':testset}"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxnfFJwBcsAv"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G6bZbbkMWWt"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, hid_dim, n_layer, act, dropout, use_bn, use_xavier):\n",
        "        super(MLP, self).__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layer = n_layer\n",
        "        self.act = act\n",
        "        self.dropout = dropout  # 0~1 float 값으로 입력받는다.\n",
        "        self.use_bn = use_bn\n",
        "        self.use_xavier = use_xavier\n",
        "        \n",
        "        # ====== Create Linear Layers ====== #\n",
        "        self.fc1 = nn.Linear(self.in_dim, self.hid_dim)\n",
        "        \n",
        "        self.linears = nn.ModuleList()\n",
        "        # BatchNorm 도 각 층마다 적용해야 되므로 층마다 각각 생성\n",
        "        self.bns = nn.ModuleList()\n",
        "        for i in range(self.n_layer-1):\n",
        "            self.linears.append(nn.Linear(self.hid_dim, self.hid_dim))\n",
        "            if self.use_bn:\n",
        "                self.bns.append(nn.BatchNorm1d(self.hid_dim))\n",
        "                \n",
        "        self.fc2 = nn.Linear(self.hid_dim, self.out_dim)\n",
        "        \n",
        "        # ====== Create Activation Function ====== #\n",
        "        if self.act == 'relu':\n",
        "            self.act = nn.ReLU()\n",
        "        elif self.act == 'tanh':\n",
        "            self.act == nn.Tanh()\n",
        "        elif self.act == 'sigmoid':\n",
        "            self.act = nn.Sigmoid()\n",
        "        else:\n",
        "            raise ValueError('no valid activation function selected!')\n",
        "        \n",
        "        # ====== Create Regularization Layer ======= #\n",
        "        # Dropout 층 생성은 무조건 함\n",
        "        # -> self.dropout = 0 으로 설정되어있으면 dropout하지 않을 것\n",
        "        self.dropout = nn.Dropout(self.dropout)\n",
        "        if self.use_xavier:\n",
        "            self.xavier_init()\n",
        "          \n",
        "    def forward(self, x):\n",
        "        x = self.act(self.fc1(x))\n",
        "        for i in range(len(self.linears)):\n",
        "            # 순서가 헷갈릴 수 있다.\n",
        "            x = self.act(self.linears[i](x))\n",
        "            x = self.bns[i](x)\n",
        "            x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "    \n",
        "    # xavier 는 주로 아래와 같이 사용한다고 한다. \n",
        "    # bias 초기화 확인\n",
        "    def xavier_init(self):\n",
        "        for linear in self.linears:\n",
        "            nn.init.xavier_normal_(linear.weight)\n",
        "            linear.bias.data.fill_(0.01)\n",
        "            \n",
        "net = MLP(3072, 10, 100, 4, 'relu', 0.1, True, True) # Testing Model Construction"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itGsp6jDWs_a"
      },
      "source": [
        "## Train, Validate, Test and Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4biT4guleN4"
      },
      "source": [
        "def train(net, partition, optimizer, criterion, args):\n",
        "    # batch size 도 파라미터로 조정하기 위해서 DataLoader도 안에서 생성\n",
        "    trainloader = torch.utils.data.DataLoader(partition['train'], \n",
        "                                              batch_size=args.train_batch_size, \n",
        "                                              shuffle=True,\n",
        "                                              num_workers=2)\n",
        "    net.train()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    train_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        optimizer.zero_grad() # [21.01.05 오류 수정] 매 Epoch 마다 .zero_grad()가 실행되는 것을 매 iteration 마다 실행되도록 수정했습니다. \n",
        "\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.view(-1, 3072)\n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = train_loss / len(trainloader)\n",
        "    train_acc = 100 * correct / total\n",
        "    return net, train_loss, train_acc"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hNCS3JpleN4"
      },
      "source": [
        "def validate(net, partition, criterion, args):\n",
        "    valloader = torch.utils.data.DataLoader(partition['val'], \n",
        "                                            batch_size=args.test_batch_size, \n",
        "                                            shuffle=False,\n",
        "                                            num_workers=2)\n",
        "    net.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0 \n",
        "    with torch.no_grad():\n",
        "        for data in valloader:\n",
        "            images, labels = data\n",
        "            images = images.view(-1, 3072)\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "            outputs = net(images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(valloader)\n",
        "        val_acc = 100 * correct / total\n",
        "    return val_loss, val_acc"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94aASgsSleN5"
      },
      "source": [
        "def test(net, partition, args):\n",
        "    testloader = torch.utils.data.DataLoader(partition['test'], \n",
        "                                             batch_size=args.test_batch_size, \n",
        "                                             shuffle=False,\n",
        "                                             num_workers=2)\n",
        "    net.eval()\n",
        "    \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images = images.view(-1, 3072)\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        test_acc = 100 * correct / total\n",
        "    return test_acc"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiOCP6TqWw2V"
      },
      "source": [
        "def experiment(partition, args):\n",
        "  \n",
        "    net = MLP(args.in_dim, args.out_dim, args.hid_dim, args.n_layer, args.act, args.dropout, args.use_bn, args.use_xavier)\n",
        "    net.cuda()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if args.optim == 'SGD':\n",
        "        optimizer = optim.SGD(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'RMSprop':\n",
        "        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'Adam':\n",
        "        optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    else:\n",
        "        raise ValueError('In-valid optimizer choice')\n",
        "    \n",
        "    for epoch in range(args.epoch):  # loop over the dataset multiple times\n",
        "        ts = time.time()\n",
        "        net, train_loss, train_acc = train(net, partition, optimizer, criterion, args)\n",
        "        val_loss, val_acc = validate(net, partition, criterion, args)\n",
        "        te = time.time()\n",
        "        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
        "        \n",
        "    test_acc = test(net, partition, args)    \n",
        "    return train_loss, val_loss, train_acc, val_acc, test_acc"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omgExzmQgU1J"
      },
      "source": [
        "## Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRoOy_B3Wu7B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8355052-afd2-4658-b396-40cb12851103"
      },
      "source": [
        "# ====== Random Seed Initialization ====== #\n",
        "seed = 123\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "\n",
        "# ====== Model Capacity ====== #\n",
        "args.in_dim = 3072\n",
        "args.out_dim = 10\n",
        "args.hid_dim = 100\n",
        "args.act = 'relu'\n",
        "\n",
        "# ====== Regularization ======= #\n",
        "args.dropout = 0.2\n",
        "args.use_bn = True\n",
        "args.l2 = 0.00001\n",
        "args.use_xavier = True\n",
        "\n",
        "# ====== Optimizer & Training ====== #\n",
        "args.optim = 'RMSprop' #'RMSprop' #SGD, RMSprop, ADAM...\n",
        "args.lr = 0.0015\n",
        "args.epoch = 10\n",
        "\n",
        "args.train_batch_size = 256\n",
        "args.test_batch_size = 1024\n",
        "\n",
        "# ====== Experiment Variable ====== #\n",
        "name_var1 = 'n_layer'\n",
        "name_var2 = 'hid_dim'\n",
        "list_var1 = [3, 3, 4]\n",
        "list_var2 = [500, 300, 700]\n",
        "\n",
        "\n",
        "for var1 in list_var1:\n",
        "    for var2 in list_var2:\n",
        "        setattr(args, name_var1, var1)\n",
        "        setattr(args, name_var2, var2)\n",
        "        print(args)\n",
        "        result = experiment(partition, args)  "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(act='relu', dropout=0.2, epoch=10, hid_dim=500, in_dim=3072, l2=1e-05, lr=0.0015, n_layer=3, optim='RMSprop', out_dim=10, test_batch_size=1024, train_batch_size=256, use_bn=True, use_xavier=True)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fd4f89e6c20>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fd4f89e6c20>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "AssertionError: can only test a child process\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fd4f89e6c20>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fd4f89e6c20>\n",
            "    self._shutdown_workers()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "    if w.is_alive():\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "    if w.is_alive():\n",
            "AssertionError: can only test a child process\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, Acc(train/val): 34.83/35.96, Loss(train/val) 1.86/1.84. Took 8.39 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fd4f89e6c20>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fd4f89e6c20>\n",
            "AssertionError: can only test a child process\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fd4f89e6c20>\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fd4f89e6c20>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "Traceback (most recent call last):\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "AssertionError: can only test a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Acc(train/val): 45.05/42.43, Loss(train/val) 1.53/1.60. Took 8.17 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fd4f89e6c20>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fd4f89e6c20>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
            "AssertionError: can only test a child process\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1320, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2, Acc(train/val): 49.33/42.08, Loss(train/val) 1.41/1.64. Took 8.21 sec\n",
            "Epoch 3, Acc(train/val): 52.46/46.56, Loss(train/val) 1.33/1.55. Took 8.24 sec\n",
            "Epoch 4, Acc(train/val): 55.44/47.58, Loss(train/val) 1.25/1.51. Took 8.14 sec\n",
            "Epoch 5, Acc(train/val): 57.68/46.92, Loss(train/val) 1.18/1.53. Took 8.07 sec\n",
            "Epoch 6, Acc(train/val): 60.34/47.97, Loss(train/val) 1.11/1.53. Took 7.98 sec\n",
            "Epoch 7, Acc(train/val): 62.16/47.26, Loss(train/val) 1.05/1.66. Took 8.23 sec\n",
            "Epoch 8, Acc(train/val): 64.46/52.20, Loss(train/val) 0.99/1.47. Took 8.12 sec\n",
            "Epoch 9, Acc(train/val): 66.54/49.96, Loss(train/val) 0.93/1.55. Took 8.00 sec\n",
            "Namespace(act='relu', dropout=0.2, epoch=10, hid_dim=300, in_dim=3072, l2=1e-05, lr=0.0015, n_layer=3, optim='RMSprop', out_dim=10, test_batch_size=1024, train_batch_size=256, use_bn=True, use_xavier=True)\n",
            "Epoch 0, Acc(train/val): 36.53/36.19, Loss(train/val) 1.77/1.87. Took 7.99 sec\n",
            "Epoch 1, Acc(train/val): 45.84/41.54, Loss(train/val) 1.51/1.67. Took 8.09 sec\n",
            "Epoch 2, Acc(train/val): 49.47/44.09, Loss(train/val) 1.41/1.56. Took 7.91 sec\n",
            "Epoch 3, Acc(train/val): 52.55/49.65, Loss(train/val) 1.32/1.43. Took 7.90 sec\n",
            "Epoch 4, Acc(train/val): 54.84/48.48, Loss(train/val) 1.26/1.48. Took 7.93 sec\n",
            "Epoch 5, Acc(train/val): 56.77/49.87, Loss(train/val) 1.20/1.46. Took 7.93 sec\n",
            "Epoch 6, Acc(train/val): 59.14/47.06, Loss(train/val) 1.14/1.54. Took 7.90 sec\n",
            "Epoch 7, Acc(train/val): 60.72/49.41, Loss(train/val) 1.10/1.48. Took 7.97 sec\n",
            "Epoch 8, Acc(train/val): 62.61/49.37, Loss(train/val) 1.05/1.50. Took 7.94 sec\n",
            "Epoch 9, Acc(train/val): 64.18/51.55, Loss(train/val) 1.00/1.46. Took 8.01 sec\n",
            "Namespace(act='relu', dropout=0.2, epoch=10, hid_dim=700, in_dim=3072, l2=1e-05, lr=0.0015, n_layer=3, optim='RMSprop', out_dim=10, test_batch_size=1024, train_batch_size=256, use_bn=True, use_xavier=True)\n",
            "Epoch 0, Acc(train/val): 33.94/36.40, Loss(train/val) 1.93/1.79. Took 8.00 sec\n",
            "Epoch 1, Acc(train/val): 43.85/39.94, Loss(train/val) 1.57/1.76. Took 7.83 sec\n",
            "Epoch 2, Acc(train/val): 48.49/37.63, Loss(train/val) 1.44/1.98. Took 8.01 sec\n",
            "Epoch 3, Acc(train/val): 51.59/42.92, Loss(train/val) 1.35/1.68. Took 8.01 sec\n",
            "Epoch 4, Acc(train/val): 53.97/45.29, Loss(train/val) 1.28/1.69. Took 8.05 sec\n",
            "Epoch 5, Acc(train/val): 56.73/49.43, Loss(train/val) 1.21/1.51. Took 8.02 sec\n",
            "Epoch 6, Acc(train/val): 59.06/47.77, Loss(train/val) 1.14/1.59. Took 7.95 sec\n",
            "Epoch 7, Acc(train/val): 61.34/46.75, Loss(train/val) 1.08/1.68. Took 8.22 sec\n",
            "Epoch 8, Acc(train/val): 63.21/48.15, Loss(train/val) 1.03/1.59. Took 8.01 sec\n",
            "Epoch 9, Acc(train/val): 65.25/46.55, Loss(train/val) 0.96/1.77. Took 8.08 sec\n",
            "Namespace(act='relu', dropout=0.2, epoch=10, hid_dim=500, in_dim=3072, l2=1e-05, lr=0.0015, n_layer=3, optim='RMSprop', out_dim=10, test_batch_size=1024, train_batch_size=256, use_bn=True, use_xavier=True)\n",
            "Epoch 0, Acc(train/val): 34.90/36.06, Loss(train/val) 1.84/1.85. Took 7.87 sec\n",
            "Epoch 1, Acc(train/val): 45.10/43.57, Loss(train/val) 1.53/1.68. Took 7.96 sec\n",
            "Epoch 2, Acc(train/val): 49.42/45.94, Loss(train/val) 1.41/1.54. Took 7.94 sec\n",
            "Epoch 3, Acc(train/val): 52.78/42.43, Loss(train/val) 1.32/1.84. Took 8.02 sec\n",
            "Epoch 4, Acc(train/val): 55.41/46.05, Loss(train/val) 1.24/1.66. Took 8.06 sec\n",
            "Epoch 5, Acc(train/val): 57.91/45.92, Loss(train/val) 1.18/1.50. Took 8.11 sec\n",
            "Epoch 6, Acc(train/val): 60.32/48.41, Loss(train/val) 1.11/1.55. Took 8.04 sec\n",
            "Epoch 7, Acc(train/val): 62.55/49.67, Loss(train/val) 1.05/1.49. Took 7.85 sec\n",
            "Epoch 8, Acc(train/val): 64.70/47.89, Loss(train/val) 0.99/1.56. Took 7.94 sec\n",
            "Epoch 9, Acc(train/val): 66.40/49.57, Loss(train/val) 0.94/1.58. Took 7.90 sec\n",
            "Namespace(act='relu', dropout=0.2, epoch=10, hid_dim=300, in_dim=3072, l2=1e-05, lr=0.0015, n_layer=3, optim='RMSprop', out_dim=10, test_batch_size=1024, train_batch_size=256, use_bn=True, use_xavier=True)\n",
            "Epoch 0, Acc(train/val): 36.06/39.05, Loss(train/val) 1.78/1.67. Took 8.14 sec\n",
            "Epoch 1, Acc(train/val): 45.74/44.45, Loss(train/val) 1.51/1.55. Took 8.08 sec\n",
            "Epoch 2, Acc(train/val): 49.53/43.35, Loss(train/val) 1.41/1.64. Took 8.03 sec\n",
            "Epoch 3, Acc(train/val): 52.33/47.11, Loss(train/val) 1.33/1.49. Took 7.96 sec\n",
            "Epoch 4, Acc(train/val): 54.50/47.26, Loss(train/val) 1.27/1.51. Took 8.11 sec\n",
            "Epoch 5, Acc(train/val): 55.79/47.13, Loss(train/val) 1.23/1.52. Took 7.94 sec\n",
            "Epoch 6, Acc(train/val): 58.68/47.40, Loss(train/val) 1.15/1.54. Took 7.95 sec\n",
            "Epoch 7, Acc(train/val): 60.26/51.38, Loss(train/val) 1.11/1.39. Took 7.94 sec\n",
            "Epoch 8, Acc(train/val): 62.11/50.38, Loss(train/val) 1.06/1.50. Took 8.06 sec\n",
            "Epoch 9, Acc(train/val): 63.73/51.07, Loss(train/val) 1.01/1.47. Took 8.00 sec\n",
            "Namespace(act='relu', dropout=0.2, epoch=10, hid_dim=700, in_dim=3072, l2=1e-05, lr=0.0015, n_layer=3, optim='RMSprop', out_dim=10, test_batch_size=1024, train_batch_size=256, use_bn=True, use_xavier=True)\n",
            "Epoch 0, Acc(train/val): 33.70/35.76, Loss(train/val) 1.92/1.85. Took 7.99 sec\n",
            "Epoch 1, Acc(train/val): 43.55/35.23, Loss(train/val) 1.57/2.03. Took 8.15 sec\n",
            "Epoch 2, Acc(train/val): 48.16/43.26, Loss(train/val) 1.44/1.65. Took 8.02 sec\n",
            "Epoch 3, Acc(train/val): 51.83/45.59, Loss(train/val) 1.35/1.59. Took 7.88 sec\n",
            "Epoch 4, Acc(train/val): 54.35/44.91, Loss(train/val) 1.27/1.57. Took 7.88 sec\n",
            "Epoch 5, Acc(train/val): 56.90/46.20, Loss(train/val) 1.21/1.53. Took 7.86 sec\n",
            "Epoch 6, Acc(train/val): 59.39/45.08, Loss(train/val) 1.13/1.80. Took 7.96 sec\n",
            "Epoch 7, Acc(train/val): 61.17/49.30, Loss(train/val) 1.08/1.55. Took 7.94 sec\n",
            "Epoch 8, Acc(train/val): 63.58/50.72, Loss(train/val) 1.01/1.56. Took 8.02 sec\n",
            "Epoch 9, Acc(train/val): 65.85/46.19, Loss(train/val) 0.95/1.90. Took 7.96 sec\n",
            "Namespace(act='relu', dropout=0.2, epoch=10, hid_dim=500, in_dim=3072, l2=1e-05, lr=0.0015, n_layer=4, optim='RMSprop', out_dim=10, test_batch_size=1024, train_batch_size=256, use_bn=True, use_xavier=True)\n",
            "Epoch 0, Acc(train/val): 33.94/37.10, Loss(train/val) 1.87/2.16. Took 7.97 sec\n",
            "Epoch 1, Acc(train/val): 44.15/42.67, Loss(train/val) 1.56/1.61. Took 7.85 sec\n",
            "Epoch 2, Acc(train/val): 48.35/39.37, Loss(train/val) 1.44/1.79. Took 7.90 sec\n",
            "Epoch 3, Acc(train/val): 51.23/46.81, Loss(train/val) 1.36/1.55. Took 7.87 sec\n",
            "Epoch 4, Acc(train/val): 53.71/47.45, Loss(train/val) 1.29/1.49. Took 8.13 sec\n",
            "Epoch 5, Acc(train/val): 56.76/46.11, Loss(train/val) 1.21/1.55. Took 7.79 sec\n",
            "Epoch 6, Acc(train/val): 58.69/47.77, Loss(train/val) 1.15/1.53. Took 7.87 sec\n",
            "Epoch 7, Acc(train/val): 60.63/50.20, Loss(train/val) 1.10/1.46. Took 7.88 sec\n",
            "Epoch 8, Acc(train/val): 62.49/47.23, Loss(train/val) 1.05/1.59. Took 7.96 sec\n",
            "Epoch 9, Acc(train/val): 64.81/45.70, Loss(train/val) 0.99/1.69. Took 8.03 sec\n",
            "Namespace(act='relu', dropout=0.2, epoch=10, hid_dim=300, in_dim=3072, l2=1e-05, lr=0.0015, n_layer=4, optim='RMSprop', out_dim=10, test_batch_size=1024, train_batch_size=256, use_bn=True, use_xavier=True)\n",
            "Epoch 0, Acc(train/val): 35.98/38.68, Loss(train/val) 1.79/1.69. Took 8.01 sec\n",
            "Epoch 1, Acc(train/val): 44.99/44.18, Loss(train/val) 1.54/1.57. Took 7.89 sec\n",
            "Epoch 2, Acc(train/val): 48.74/45.97, Loss(train/val) 1.43/1.51. Took 8.23 sec\n",
            "Epoch 3, Acc(train/val): 51.46/42.12, Loss(train/val) 1.35/1.66. Took 8.21 sec\n",
            "Epoch 4, Acc(train/val): 54.05/48.12, Loss(train/val) 1.28/1.49. Took 8.06 sec\n",
            "Epoch 5, Acc(train/val): 56.38/49.99, Loss(train/val) 1.22/1.42. Took 7.90 sec\n",
            "Epoch 6, Acc(train/val): 58.64/48.90, Loss(train/val) 1.16/1.51. Took 8.13 sec\n",
            "Epoch 7, Acc(train/val): 60.23/47.06, Loss(train/val) 1.11/1.60. Took 8.10 sec\n",
            "Epoch 8, Acc(train/val): 62.05/50.73, Loss(train/val) 1.06/1.47. Took 7.89 sec\n",
            "Epoch 9, Acc(train/val): 63.58/48.90, Loss(train/val) 1.02/1.55. Took 8.03 sec\n",
            "Namespace(act='relu', dropout=0.2, epoch=10, hid_dim=700, in_dim=3072, l2=1e-05, lr=0.0015, n_layer=4, optim='RMSprop', out_dim=10, test_batch_size=1024, train_batch_size=256, use_bn=True, use_xavier=True)\n",
            "Epoch 0, Acc(train/val): 32.80/33.25, Loss(train/val) 1.97/1.95. Took 8.38 sec\n",
            "Epoch 1, Acc(train/val): 42.55/37.76, Loss(train/val) 1.59/1.91. Took 8.18 sec\n",
            "Epoch 2, Acc(train/val): 47.28/40.22, Loss(train/val) 1.47/1.79. Took 8.09 sec\n",
            "Epoch 3, Acc(train/val): 50.48/41.94, Loss(train/val) 1.38/1.69. Took 8.03 sec\n",
            "Epoch 4, Acc(train/val): 53.71/44.61, Loss(train/val) 1.30/1.63. Took 7.92 sec\n",
            "Epoch 5, Acc(train/val): 56.06/44.16, Loss(train/val) 1.23/1.63. Took 7.96 sec\n",
            "Epoch 6, Acc(train/val): 58.22/49.07, Loss(train/val) 1.17/1.50. Took 7.87 sec\n",
            "Epoch 7, Acc(train/val): 60.33/45.05, Loss(train/val) 1.10/1.64. Took 7.79 sec\n",
            "Epoch 8, Acc(train/val): 62.85/46.34, Loss(train/val) 1.04/1.70. Took 8.07 sec\n",
            "Epoch 9, Acc(train/val): 64.74/48.53, Loss(train/val) 0.99/1.54. Took 7.87 sec\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}